{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text can be seen as following:\n",
    "* Sequence of characters\n",
    "* Sequence of words\n",
    "* Sequence of N-grams\n",
    "* Most common to work at the level of words\n",
    "\n",
    "Using this representation of text, most of the models understand the statistical structure of text i.e. identify various features from the text which can solve some of the textual tasks e.g. document classification, author identificaiton, sentiment analysis etc. At high level, various algorithms treat the sequence elements similarly to how pixels are treated in context of images in computere vision.\n",
    "\n",
    "### Numerical representation of text as sequence\n",
    "All algorithms works on numerial tensor, so we need to transform the raw text sequence into numerical tensor.\n",
    "\n",
    "#### Tokenization\n",
    "A text can be split in lower form by splitting into seq. of char or words or n-grams. This process of splitting is called tokenization and individual elements like char, words or n-gram is called token. And then each token can be converted into a numerical vector:\n",
    "\n",
    "* Sequence of chars: treat each char in the sequence as a vector and using that create a tensor for the whole sequence of char\n",
    "* Seq. of words: treat each word as a vector and using that create a tensor for a single sentence.\n",
    "* Split the sentence using n-gram(take consecutive n chars as a single entity) and treat each n-gram as a vector and then create a numerial tensor out of it.\n",
    "\n",
    "\n",
    "#### Vector representation\n",
    "* One hot encoding\n",
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "* It is the most basic way to transform a token into a vector\n",
    "* Total no. of dimension = no. of unique tokens in vocabulary\n",
    "* Assign each token a unique dimension and then represent that token by a vector with value 1 at the assigned dimension and 0 in other dimension.\n",
    "* Here the tokens can be chars, words, n-grams\n",
    "* Vectors are binary, sparse (mostly made of 0), high dimensional e.g. 20,000 dimensional for data having that much unique tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(10)\n",
    "sample_texts = ['This is a car.', 'That is a bicycle']\n",
    "tokenizer.fit_on_texts(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4], [5, 1, 2, 6]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "print(sequences)\n",
    "# each of the words/token is assigned an integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "one_hot_encoding_result = tokenizer.texts_to_matrix(sample_texts)\n",
    "print(one_hot_encoding_result)\n",
    "# each word present as 1, order is lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1, 'a': 2, 'this': 3, 'car': 4, 'that': 5, 'bicycle': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "* More powerful way of representing tokens as vectors\n",
    "* Here tokens are words\n",
    "* Embeddings are learned from data\n",
    "* Word embedding are low dimensional floating point vectors. Generally used dimensions are: 256, 512, 1024..\n",
    "* Two ways:\n",
    "    * Learn with the main task\n",
    "        * Start with random vector for a token and then learn\n",
    "    * Load pre trained word embedings computed in different task\n",
    "* Features:\n",
    "    * Geometric relationship with vectors, should reflect semantic relationship between words\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visual representation of one hot encodding and word embedding](img/word-representation-dlipbook.png)\n",
    "\n",
    "Source: Deep Learning with Python by Francois Chollet, Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//todo \n",
    "add code snippet for loading pre computed word embeddings\n",
    "page 186 def.\n",
    "Word index --> Embedding layer --> Corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding = Embedding(11, 2) # 11 = 10 (max no. of word index declared above) + 1; 2 = embedding dimension\n",
    "#input to embedding layer will be word sequence (sequences created above - word index vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pre trained embedding\n",
    "Generally if we do not have enough data for training then we use pretrained word embeddings.  \n",
    "Following are some of the most used pre computed word embeddings:\n",
    "* Word2Vec: which captures specific semantic structure  \n",
    "https://code.google.com/archive/p/word2vec\n",
    "* GloVe: which captures co-occurence statistics for millions of English tokens from Wikipedia and Common Crawl data.  \n",
    "https://nlp.stanford.edu/projects/glove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other reading materials: \n",
    "* Lec 2 & 3 from http://web.stanford.edu/class/cs224n/syllabus.html\n",
    "* http://ruder.io/word-embeddings-1/\n",
    "* http://ruder.io/word-embeddings-softmax/\n",
    "\n",
    "Course video: \n",
    "* [Lecture 2 | Word Vector Representations: word2vec](https://youtu.be/ERibwqs9p38)\n",
    "* [Lecture 3 | GloVe: Global Vectors for Word Representation](https://youtu.be/ASn7ExxLZws)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
