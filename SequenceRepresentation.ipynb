{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation \n",
    "##### Text can be seen as following:\n",
    "* Sequence of characters\n",
    "* Sequence of words\n",
    "* Sequence of N-grams\n",
    "\n",
    "Most common to work at the level of words.\n",
    "\n",
    "Using this representation of text, most of the models understand the statistical structure of text i.e. identify various features from the text which can solve some of the textual tasks e.g. document classification, author identificaiton, sentiment analysis etc. At high level, various algorithms treat the sequence elements similarly to how pixels are treated in context of images in computere vision.\n",
    "\n",
    "#### Numerical representation of text as sequence\n",
    "All algorithms works on numerial tensor, so we need to transform the raw text sequence into numerical tensor. And this is done in 2 steps:\n",
    "\n",
    "##### Step 1: Tokenization\n",
    "A text can be split in lower form by splitting into seq. of char or words or n-grams. This process of splitting is called tokenization and individual elements like char, words or n-gram is called token. And then each token can be converted into a numerical vector:\n",
    "\n",
    "* Sequence of chars: treat each char in the sequence as a vector and using that create a tensor for the whole sequence of char\n",
    "* Seq. of words: treat each word as a vector and using that create a tensor for a single sentence.\n",
    "* Split the sentence using n-gram(take consecutive n chars as a single entity) and treat each n-gram as a vector and then create a numerial tensor out of it.\n",
    "\n",
    "\n",
    "##### Step 2: Vector representation\n",
    "The above generated tokens can be transformed into numerical vector using following representation:\n",
    "* One hot encoding\n",
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "* It is the most basic way to transform a token into a vector\n",
    "* Total no. of dimension = no. of unique tokens in vocabulary\n",
    "* Assign each token a unique dimension and then represent that token by a vector with value 1 at the assigned dimension and 0 in other dimension.\n",
    "* Here the tokens can be chars, words, n-grams\n",
    "* Vectors are binary, sparse (mostly made of 0), high dimensional e.g. 20,000 dimensional for data having that much unique tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample keras code for One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_num_words = 10\n",
    "tokenizer = Tokenizer(max_num_words)\n",
    "sample_texts = ['This is a car.', 'That is a bicycle']\n",
    "tokenizer.fit_on_texts(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1, 'a': 2, 'this': 3, 'car': 4, 'that': 5, 'bicycle': 6}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 1, 2, 4], [5, 1, 2, 6]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "print(sequences)\n",
    "# each of the words/token is assigned an integer value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "one_hot_encoded = to_categorical(sequences, num_classes=max_num_words)\n",
    "print(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "bag_of_word_encoding = tokenizer.texts_to_matrix(sample_texts)\n",
    "print(bag_of_word_encoding)\n",
    "# each word present as 1, order is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, 1st vector is:\n",
    "\\[0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\\]\n",
    "And 1st sentence is: 'This is a car' if we check each token(word) index, it is 1 in the encoding at that location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "* More powerful way of representing tokens as vectors\n",
    "* Here tokens are words\n",
    "* Word embeddings are learned from data\n",
    "* Word embeddings are low dimensional floating point vectors. Generally used dimensions are: 256, 512, 1024..\n",
    "* Two ways:\n",
    "    * Learn with the main task: start with random vector for a token and then learn the embeddings\n",
    "    * Load pre trained word embedings computed in different task\n",
    "* Features:\n",
    "    * Geometric relationship with vectors, should reflect semantic relationship between words\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Visual representation of one hot encodding and word embedding](img/word-representation-dlwpbook.png)\n",
    "\n",
    "Source: Deep Learning with Python by Francois Chollet, Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras lib. has embeddding layer\n",
    "The Embedding layer maps word's integer indices to dense vectors\n",
    "Word index --> Embedding layer --> Corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding = Embedding(11, 2) \n",
    "# 11 = input dimension = 10 (max no. of word index declared above) + 1\n",
    "# 2  = output dimension = embedding dimension\n",
    "# input to embedding layer will be word sequence (sequences created above - word index vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to Embedding layer is (samples, sequence length)  \n",
    "Output of Embedding layer is (samples, sequence length, embedding dimension)\n",
    "\n",
    "Weights of embedding layer random assigned and learning/adjusted via backpropagation.\n",
    "\n",
    "Word embeddings trained from one data can be used in other problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre trained embeddings\n",
    "Generally if we do not have enough data for training then we use pretrained word embeddings.  \n",
    "Following are some of the most used pre computed word embeddings:\n",
    "* Word2Vec: which captures specific semantic structure  \n",
    "https://code.google.com/archive/p/word2vec\n",
    "* GloVe: which captures co-occurence statistics for millions of English tokens from Wikipedia and Common Crawl data.  \n",
    "https://nlp.stanford.edu/projects/glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code snippet to load word embeddings\n",
    "We will load the GloVe word embeddings. Download the precomputed glove embedding on wikipedia data from above link and extract in ./data/glove/glove.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "glove_dir = 'data/glove/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is sample code which will not execute properly here, its a code snippet for specific use case\n",
    "word_index = tokenizer.word_index # and this tokenizer is trained/fit on training data\n",
    "max_words = len(word_index)\n",
    "embedding_dim = 100 # dimension of output of embedding layer to be, its 100 as we are using pre trained with 100\n",
    "embedding_matrix = np.zeros((max_words + 1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code snippet to load weights\n",
    "# Support in a model an Embedding layer is added, which can be added only as a 1st layer of the model.\n",
    "# Then we can load this embedding as weights of that layer\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other reading materials: \n",
    "* Lec 2 & 3 from https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/syllabus.html\n",
    "* http://ruder.io/word-embeddings-1/\n",
    "* http://ruder.io/word-embeddings-softmax/\n",
    "\n",
    "Course video: \n",
    "* [Lecture 2 | Word Vector Representations: word2vec](https://youtu.be/ERibwqs9p38)\n",
    "* [Lecture 3 | GloVe: Global Vectors for Word Representation](https://youtu.be/ASn7ExxLZws)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
