{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Model 1 can be represented as follow:\n",
    "* Input => Embedding => Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps for training:\n",
    "* Load Data\n",
    "    * Train Data\n",
    "* Pre-processing: Tokenization\n",
    "* Batching and Padding\n",
    "* Model definition\n",
    "* Training and valildation\n",
    "* Evaluation\n",
    "* Excercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the toxic comment classification challenge dataset\n",
    "and split the dataset into training, validation, testing\n",
    "\n",
    "#### Data for training\n",
    "For training, we need dataset in 2 groups (pair: comment and its corresponding output label):\n",
    "1. __Input data:__ wikipedia comments\n",
    "2. __Output label:__ whether the comment is toxic or not\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV\n",
    "* read the csv data file using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_csv = './storage/dataset/train.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "# To Do: sort the df based on size of comments (no. of words in comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data Preperation\n",
    "* read the labels and convert into one-class labels\n",
    "* we will focus on 2 class problem: toxic and non toxic comments\n",
    "* we will label all different types of toxic comments into same category of toxic label:\n",
    "    * 0 for toxic comment\n",
    "    * 1 for non-toxic comments\n",
    "* later we can explore how to make it multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each toxic class is labelled as 1\n",
    "toxic_row_sums = train_df.iloc[:,2:].sum(axis=1)\n",
    "# if sum of toxic class is 0 then it is a clean comment\n",
    "train_df['clean'] = (toxic_row_sums==0)\n",
    "# Input Data\n",
    "train_texts = train_df['comment_text']\n",
    "# Output Label\n",
    "train_labels = train_df['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing : Tokenization\n",
    "Now we have training data in two separate dataframe columns (arrays/list): an ordered array consisting of comments (input for the network) and another array consisting of class lables in same order (output of the network).\n",
    "\n",
    "We have to transform this data into network input format and output format. This step is called pre-processing.  \n",
    "Steps of pre-processing:\n",
    "\n",
    "1. Tokenize the text into words\n",
    "2. Assign each word a dimension\n",
    "\n",
    "\n",
    "To accompolish step 1 and 2 we will use inbuilt __Tokenizer__ class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 1116, 86, 331, 51, 2278, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n",
      "Found 210337 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# set size of vocabulary\n",
    "# To Do: try different size \n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Padding for Embedding\n",
    "Now once we have the tokens and each token(word) has a dimension assigned to it, we will do following steps to create word embeddings  \n",
    "\n",
    "3. use this dimension assignments to define embedding for individual word\n",
    "4. use word embedding to create word vector for a comment\n",
    "\n",
    "\n",
    "We will use a specific type of network layer for this, which is called __Embedding Layer__. The above generated tokens (sequence of number) will go as input to Embedding layer, which will output word embeddings as output to next layer.  \n",
    "\n",
    "Input and Output of Neural Network are done is batches. A batch is a group of input data which are fed together to the network. As the network can process individual data element in parallel, the training will be faster.\n",
    "\n",
    "In case of Embedding Layer, Inpupt and Output in a batch can be seen as follows:  \n",
    "\n",
    "   **Input**: 2D tensor of integers, of shape (# seq. samples in particular batch, sequence_length), where each entry is a sequence of integers (output of above code).  \n",
    "   **Output**: 3D floating-point tensor of shape (# seq. samples in particula patch, sequence_length, embedding_dimensionality).  \n",
    "\n",
    "Sequence length can be variable per batch. But in a single batch sequence length will be same for all sequences.  \n",
    "\n",
    "So from data we have to create batches of sequence of similar length and to do that we have to pad or truncate each sequence to have same sequence length. And we can use each batch as a training input for embedding layer.  \n",
    "\n",
    "For sample case: we take 10k sequence from 160k for training in a single batch. And take max sequence length of 20 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "training_sequences = sequences[:10000]\n",
    "training_labels = train_labels[:10000]\n",
    "seq_max_len = 20\n",
    "# training padded sequences\n",
    "train_seq_pad = preprocessing.sequence.pad_sequences(sequences=training_sequences, maxlen=seq_max_len)\n",
    "\n",
    "# testing padded sequences\n",
    "testing_sequences = sequences[10000:11000]\n",
    "testing_labels = train_labels[10000:11000]\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=testing_sequences, maxlen=seq_max_len)\n",
    "\n",
    "# To Do: try more training data, try different sequence max length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. : Embedding to Class\n",
    "\n",
    "#### Define the model 1\n",
    "Model 1 is made of 4 layers:\n",
    "    - Layer 0 is input layer\n",
    "    - Layer 1 is Embedding layer (Hidden Layer)\n",
    "    - Layer 2 is Flatten Layer (Flattens the embedding layer)\n",
    "    - Layer 3 is Dense Layer (output layer)\n",
    "    \n",
    "**Embedding Layer**: This layer help us create word embedding (discussed in Sequence Representation section). For a single input (a sentence which comes as a seq. of integer) its output is 2D. Each integer(representing a word) gets transformed into a vector; so for a seq. of int. it generates a 2D matrix.\n",
    "\n",
    "**Flatten Layer**: Embedding layer outputs in 2D matrix, to use the output in a Dense layer upstream the output need to transformed into 1D and flatten layer does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "# no. of unique words in the text data, each word in vocab will be assigned an index (dimension).\n",
    "# = max_vocab_size defined above\n",
    "vocab_size = 10000 \n",
    "\n",
    "# max length of single input data point i.e. count of words present in an input sentence\n",
    "# short seq are padded and long ones are truncated, done above\n",
    "# sequence size of single input for the network\n",
    "seq_max_len = 20 \n",
    "\n",
    "# dimension of word embedding model (output dimension of embedding layer)\n",
    "embedding_dim = 8 \n",
    "\n",
    "## layer 1: add Embedding Layer in the network\n",
    "#  input to this layer is data of shape: [batch_size, seq_max_len]\n",
    "model_1.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "#  output of layer 1 is data of shape: [batch_size, embedding_dim, seq_max_len]\n",
    "\n",
    "## layer 2: flatten the input of shape [batch_size, embedding_dim, seq_max_len] \n",
    "model_1.add(Flatten())\n",
    "#  output of shape [batch_size, embedding_dimension x seq_max_len]\n",
    "\n",
    "## layer 3 (final/output layer): Dense layer \n",
    "#  all nodes from previous layers are connected to each nodes from this layer\n",
    "#  this has 1 unit/node for classification(toxic/non-toxic)\n",
    "#  and activation for 2 classes: sigmoind\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "## compile:   configure the model for training\n",
    "#  optimizer: it is the method use to update the network, \n",
    "#             it is generally variant of stochastic gradient descent (SGD)  \n",
    "#             this method is use iteratively to update the network weights\n",
    "#  loss:      it is the (objective) function that will be minimised\n",
    "#  metrics:   this is use to measure the performance of network\n",
    "model_1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# prints the summary of the model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.5771 - acc: 0.8711 - val_loss: 0.4365 - val_acc: 0.9100\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.3622 - acc: 0.8941 - val_loss: 0.2896 - val_acc: 0.9100\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 31us/step - loss: 0.2891 - acc: 0.8946 - val_loss: 0.2587 - val_acc: 0.9105\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.2583 - acc: 0.8966 - val_loss: 0.2417 - val_acc: 0.9125\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 33us/step - loss: 0.2304 - acc: 0.9023 - val_loss: 0.2267 - val_acc: 0.9155\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 36us/step - loss: 0.2047 - acc: 0.9116 - val_loss: 0.2149 - val_acc: 0.9220\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 24us/step - loss: 0.1832 - acc: 0.9236 - val_loss: 0.2071 - val_acc: 0.9300\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 27us/step - loss: 0.1658 - acc: 0.9335 - val_loss: 0.2023 - val_acc: 0.9325\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 26us/step - loss: 0.1519 - acc: 0.9385 - val_loss: 0.1994 - val_acc: 0.9350\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 25us/step - loss: 0.1404 - acc: 0.9435 - val_loss: 0.1976 - val_acc: 0.9345\n"
     ]
    }
   ],
   "source": [
    "# fit: trains the network for a fixed no. of epoch\n",
    "history_1 = model_1.fit(train_seq_pad, training_labels, epochs=10, batch_size=100, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/m11.png\" alt=\"Visual representation of one hot encodding and word embedding\" style=\"width: 600px;\"/>  \n",
    "\n",
    "\n",
    "Created using [NN SVG Tool](http://alexlenail.me/NN-SVG/index.html)  \n",
    "\n",
    "For above diagram, following configs are used(1/4th of the ones used in code):\n",
    "1. seq_max_len = 5\n",
    "2. embedding_dim = 2\n",
    "3. flatten layer = 5x2 = 10\n",
    "4. desnse output layer = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model 1\n",
    "\n",
    "We will take a small test data from the unused training data to test our basic model.  \n",
    "\n",
    "`model_1.evaluate` method is use to evaluate the model. For evaluation we give input the test data in the same format as of training data together with label data for the test data to compare with.\n",
    "\n",
    "Ref: Listing 6.7 Deep Learning with Python book  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1935289661884308, 0.92900000000000005]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1.metrics_names)\n",
    "model_1.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = './storage/dataset/test.csv'\n",
    "test_csv_df = pd.read_csv(test_csv)\n",
    "test_csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data\n",
    "test_csv_texts = test_csv_df['comment_text']\n",
    "test_csv_texts_sequences = tokenizer.texts_to_sequences(test_csv_texts)\n",
    "testing_csv_sequences = test_csv_texts_sequences[:1000]\n",
    "seq_max_len = 20\n",
    "# training padded sequences\n",
    "test_csv_seq_pad = preprocessing.sequence.pad_sequences(sequences=testing_csv_sequences, maxlen=seq_max_len)\n",
    "# make a prediction\n",
    "ynew = model_1.predict_classes(test_csv_seq_pad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the inputs and predicted outputs\n",
    "for i in range(len(test_csv_seq_pad)):\n",
    "    print(\"Predicted=%s : Comment=%s\" % (ynew[i], test_csv_texts[i]))\n",
    "    print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercises\n",
    "Try to improve performance of the model:\n",
    "* Sort comments after reading CSV file, to group comments of similar size in a batch\n",
    "* Try different vocab size during tokenization e.g. set size dynamically based on some logic e.g. select top 90% frequent words or words with frequency more than some value\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
