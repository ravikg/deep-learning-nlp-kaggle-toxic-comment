{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complex Models\n",
    "\n",
    "We will cover following models:\n",
    "* Embedding => LSTM => Class\n",
    "* Embedding => GRU => Class\n",
    "* Embedding => Seq2Seq => Class\n",
    "* Embedding => Seq2Seq with Attention => Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, tokenize and embedding of data\n",
    "For details of this section please see [Models Notebook](Models.iynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import pandas as pd\n",
    "train_csv = './data/toxic-comments/train.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "\n",
    "rowsums=train_df.iloc[:,2:].sum(axis=1)\n",
    "train_df['clean']=(rowsums==0)\n",
    "train_texts = train_df['comment_text']\n",
    "train_labels = train_df['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# tokenization\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batching, pre-processing for embedding layer\n",
    "from keras import preprocessing\n",
    "training_sequences = sequences[:10000]\n",
    "training_labels = train_labels[:10000]\n",
    "seq_max_len = 20\n",
    "# training padded sequences\n",
    "train_seq_pad = preprocessing.sequence.pad_sequences(sequences=training_sequences, maxlen=seq_max_len)\n",
    "\n",
    "# testing padded sequences\n",
    "testing_sequences = sequences[10000:11000]\n",
    "testing_labels = train_labels[10000:11000]\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=testing_sequences, maxlen=seq_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. : Embedding to LSTML to Class\n",
    "\n",
    "#### Define the model 1\n",
    "Model 1 is made of 2 layers:\n",
    "    - Layer 1 is Embedding layer\n",
    "    - Layer 2 is LSTM layer\n",
    "\n",
    "- Layer 2 is classification (Dense) Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               70144     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 150,273\n",
      "Trainable params: 150,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 3s 430us/step - loss: 0.3126 - acc: 0.8959 - val_loss: 0.2425 - val_acc: 0.9210\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 3s 404us/step - loss: 0.2150 - acc: 0.9237 - val_loss: 0.2346 - val_acc: 0.9290\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 3s 386us/step - loss: 0.1725 - acc: 0.9410 - val_loss: 0.2531 - val_acc: 0.9315\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 3s 385us/step - loss: 0.1544 - acc: 0.9477 - val_loss: 0.2064 - val_acc: 0.9400\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.1361 - acc: 0.9509 - val_loss: 0.2008 - val_acc: 0.9445\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 3s 389us/step - loss: 0.1269 - acc: 0.9559 - val_loss: 0.2293 - val_acc: 0.9405\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.1170 - acc: 0.9611 - val_loss: 0.2180 - val_acc: 0.9270\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 3s 397us/step - loss: 0.1088 - acc: 0.9636 - val_loss: 0.1978 - val_acc: 0.9450\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 3s 388us/step - loss: 0.1028 - acc: 0.9649 - val_loss: 0.2405 - val_acc: 0.9405\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 3s 393us/step - loss: 0.0977 - acc: 0.9669 - val_loss: 0.2103 - val_acc: 0.9395\n"
     ]
    }
   ],
   "source": [
    "# for details about layer 1 and layer 3 code, please check Models.ipynb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "vocab_size = 10000 \n",
    "embedding_dim = 8 \n",
    "seq_max_len = 20 \n",
    "model_1.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "#LSTM: dimentionality\n",
    "model_1.add(LSTM(128))\n",
    "\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model_1.summary()\n",
    "\n",
    "history_1 = model_1.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 132us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.21064355981349944, 0.9300000071525574]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1.metrics_names)\n",
    "model_1.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extend the model by adding more LSTM layers in between and for the intermediate layers need to return output sequence for next layer.\n",
    "\n",
    "#### Extended model 2\n",
    "Extended model 2 is made of 5 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is LSTM RNN layer (return full sequence)\n",
    "- Layer 3 is LSTM RNN layer (return full sequence)\n",
    "- Layer 4 is LSTM RNN layer (return last output)\n",
    "- Layer 5 is classification (Dense) layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, None, 32)          6272      \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, None, 64)          24832     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,553\n",
      "Trainable params: 203,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1_ext = Sequential()\n",
    "model_1_ext.add(Embedding(vocab_size, embedding_dim))\n",
    "# for intermediate layers, we want to return output of each cell of the RNN, \n",
    "# so that it forms a seq. which is processed by next RNN layer\n",
    "model_1_ext.add(LSTM(32, return_sequences=True))\n",
    "model_1_ext.add(LSTM(64, return_sequences=True))\n",
    "# in final RNN layer we will not return the sequence but only the final cell output,\n",
    "# which is use in the next non RNN layer e.g. Dense layer in this case\n",
    "model_1_ext.add(LSTM(32))\n",
    "model_1_ext.add(Dense(1, activation='sigmoid'))\n",
    "model_1_ext.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_1_ext.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the extended model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 5s 664us/step - loss: 0.3050 - acc: 0.8953 - val_loss: 0.2459 - val_acc: 0.9195\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 4s 547us/step - loss: 0.2082 - acc: 0.9241 - val_loss: 0.2007 - val_acc: 0.9315\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 4s 540us/step - loss: 0.1660 - acc: 0.9425 - val_loss: 0.1983 - val_acc: 0.9370\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 4s 542us/step - loss: 0.1400 - acc: 0.9509 - val_loss: 0.1892 - val_acc: 0.9420\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 4s 543us/step - loss: 0.1237 - acc: 0.9576 - val_loss: 0.1859 - val_acc: 0.9440\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 4s 540us/step - loss: 0.1093 - acc: 0.9629 - val_loss: 0.1922 - val_acc: 0.9435\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 4s 544us/step - loss: 0.0988 - acc: 0.9663 - val_loss: 0.1883 - val_acc: 0.9495\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 4s 540us/step - loss: 0.0919 - acc: 0.9694 - val_loss: 0.2107 - val_acc: 0.9320\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 4s 541us/step - loss: 0.0865 - acc: 0.9691 - val_loss: 0.2167 - val_acc: 0.9220\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 4s 543us/step - loss: 0.0808 - acc: 0.9724 - val_loss: 0.2180 - val_acc: 0.9285\n"
     ]
    }
   ],
   "source": [
    "history_1_ext = model_1_ext.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the extended model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 159us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2308857423812151, 0.925000011920929]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1_ext.metrics_names)\n",
    "model_1_ext.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding => GRU => Class\n",
    "In this model 2 we will update the Model 1 by replacing LSTM layer by GRU.\n",
    "\n",
    "\n",
    "#### Define the model 2\n",
    "Model 2 is made of 3 layers:\n",
    "    - Layer 1 is Embedding layer\n",
    "    - Layer 2 is GRU layer\n",
    "    - Layer 3 is classification (Dense) layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                4704      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 164,737\n",
      "Trainable params: 164,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, GRU\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16\n",
    "\n",
    "# model definition\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "model_2.add(GRU(32))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 284us/step - loss: 0.3066 - acc: 0.8951 - val_loss: 0.2210 - val_acc: 0.9215\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.1964 - acc: 0.9255 - val_loss: 0.2025 - val_acc: 0.9400\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.1593 - acc: 0.9411 - val_loss: 0.1954 - val_acc: 0.9380\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2s 231us/step - loss: 0.1335 - acc: 0.9538 - val_loss: 0.1899 - val_acc: 0.9475\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.1202 - acc: 0.9585 - val_loss: 0.1935 - val_acc: 0.9445\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.1119 - acc: 0.9620 - val_loss: 0.1886 - val_acc: 0.9505\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.1056 - acc: 0.9643 - val_loss: 0.2028 - val_acc: 0.9405\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2s 233us/step - loss: 0.1025 - acc: 0.9657 - val_loss: 0.2183 - val_acc: 0.9375\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.0984 - acc: 0.9675 - val_loss: 0.2074 - val_acc: 0.9440\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s 235us/step - loss: 0.0961 - acc: 0.9678 - val_loss: 0.2022 - val_acc: 0.9365\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 73us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22840861797332765, 0.9240000247955322]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2.metrics_names)\n",
    "model_2.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that above model didn't have good accuracy compared to much simpler model. We didn't use most of the data, training data is very less and also value of seq_len was less for training data and more for testing data.\n",
    "\n",
    "\n",
    "We can extend the model by adding more RNN layers in between and for the above we didn't use the out of intermediate output of RNN layer.\n",
    "\n",
    "#### Extended model 2\n",
    "Extended model 2 is made of 5 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is GRU RNN layer (return full sequence)\n",
    "- Layer 3 is GRU RNN layer (return full sequence)\n",
    "- Layer 4 is GRU RNN layer (return last output)\n",
    "- Layer 5 is classification (Dense) layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, None, 32)          4704      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, None, 64)          18624     \n",
      "_________________________________________________________________\n",
      "gru_4 (GRU)                  (None, 32)                9312      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 192,673\n",
      "Trainable params: 192,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2_ext = Sequential()\n",
    "model_2_ext.add(Embedding(vocab_size, embedding_dim))\n",
    "# for intermediate layers, we want to return output of each cell of RNN, \n",
    "# so that it forms a seq. which is processed by next RNN layer\n",
    "model_2_ext.add(GRU(32, return_sequences=True))\n",
    "model_2_ext.add(GRU(64, return_sequences=True))\n",
    "# in final RNN layer we will not return the sequence but only the final output,\n",
    "# which is use in the next non RNN layer e.g. Dense layer in this case\n",
    "model_2_ext.add(GRU(32))\n",
    "model_2_ext.add(Dense(1, activation='sigmoid'))\n",
    "model_2_ext.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_2_ext.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the extended model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 6s 749us/step - loss: 0.2794 - acc: 0.9047 - val_loss: 0.2170 - val_acc: 0.9300\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 5s 598us/step - loss: 0.1798 - acc: 0.9369 - val_loss: 0.1797 - val_acc: 0.9410\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 5s 601us/step - loss: 0.1425 - acc: 0.9515 - val_loss: 0.1734 - val_acc: 0.9485\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 5s 592us/step - loss: 0.1258 - acc: 0.9586 - val_loss: 0.1706 - val_acc: 0.9465\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 5s 595us/step - loss: 0.1116 - acc: 0.9638 - val_loss: 0.1867 - val_acc: 0.9395\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 5s 596us/step - loss: 0.1034 - acc: 0.9646 - val_loss: 0.1866 - val_acc: 0.9475\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 5s 589us/step - loss: 0.0986 - acc: 0.9672 - val_loss: 0.1939 - val_acc: 0.9300\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 5s 584us/step - loss: 0.0939 - acc: 0.9684 - val_loss: 0.2101 - val_acc: 0.9465\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 5s 606us/step - loss: 0.0925 - acc: 0.9682 - val_loss: 0.2228 - val_acc: 0.9260\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 5s 604us/step - loss: 0.0880 - acc: 0.9705 - val_loss: 0.2008 - val_acc: 0.9320\n"
     ]
    }
   ],
   "source": [
    "history_2_ext = model_2_ext.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the extended model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.22118819653987884, 0.9269999861717224]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2_ext.metrics_names)\n",
    "model_2_ext.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Embedding => Bidirectional RNN => Output\n",
    "In this model 3 we will extend the Model 2 by wrapping the RNN layer with a Bidirectional wrapper.\n",
    "\n",
    "#### Define the model 3\n",
    "Extended model 3 is made of 3 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is Bidirectional RNN layer (return last output)\n",
    "- Layer 3 is classification (Dense) layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 163,201\n",
      "Trainable params: 163,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16\n",
    "\n",
    "# model definition\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "# [1] This will create two copies of the hidden layer, \n",
    "# one fit in the input sequences as-is and one on a reversed copy of the input sequence. \n",
    "# By default, the output values from these LSTMs will be concatenated.\n",
    "model_3.add(Bidirectional(SimpleRNN(32)))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 229us/step - loss: 0.3031 - acc: 0.8986 - val_loss: 0.2260 - val_acc: 0.9225\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2s 193us/step - loss: 0.2001 - acc: 0.9320 - val_loss: 0.1998 - val_acc: 0.9360\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.1465 - acc: 0.9504 - val_loss: 0.2050 - val_acc: 0.9270\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.1048 - acc: 0.9641 - val_loss: 0.2162 - val_acc: 0.9325\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.0716 - acc: 0.9759 - val_loss: 0.2518 - val_acc: 0.9160\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.0424 - acc: 0.9866 - val_loss: 0.2988 - val_acc: 0.9085\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 0.2838 - val_acc: 0.9285\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.3168 - val_acc: 0.9115\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.0127 - acc: 0.9955 - val_loss: 0.3652 - val_acc: 0.9115\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.0065 - acc: 0.9977 - val_loss: 0.4414 - val_acc: 0.8815\n"
     ]
    }
   ],
   "source": [
    "history_3 = model_3.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 58us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4578404839038849, 0.8730000257492065]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_3.metrics_names)\n",
    "model_3.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly like model 2, model 3 can be extended by adding more bidirectional layers in between.  \n",
    "\n",
    "#### Extended model 3\n",
    "Extended model 3 is made of 5 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is Bidirectional RNN layer (return full sequence)\n",
    "- Layer 3 is Bidirectional RNN layer (return full sequence)\n",
    "- Layer 4 is Bidirectional RNN layer (return last output)\n",
    "- Layer 5 is classification (Dense) layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 64)            3136      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 20, 128)           16512     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                10304     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 190,017\n",
      "Trainable params: 190,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "model_3_ext = Sequential()\n",
    "model_3_ext.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(32, return_sequences=True)))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(64, return_sequences=True)))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(32)))\n",
    "model_3_ext.add(Dense(1, activation='sigmoid'))\n",
    "model_3_ext.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_3_ext.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train ext. model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/code/virtualenvs/deep-learn/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 5s 587us/step - loss: 0.3033 - acc: 0.8999 - val_loss: 0.2239 - val_acc: 0.9260\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 4s 462us/step - loss: 0.1896 - acc: 0.9352 - val_loss: 0.2331 - val_acc: 0.9285\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 3s 424us/step - loss: 0.1191 - acc: 0.9599 - val_loss: 0.2363 - val_acc: 0.9290\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 4s 439us/step - loss: 0.0698 - acc: 0.9768 - val_loss: 0.2847 - val_acc: 0.9250\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 3s 419us/step - loss: 0.0413 - acc: 0.9879 - val_loss: 0.3350 - val_acc: 0.9110\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 3s 416us/step - loss: 0.0236 - acc: 0.9930 - val_loss: 0.3737 - val_acc: 0.9065\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.5692 - val_acc: 0.8740\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 3s 415us/step - loss: 0.0075 - acc: 0.9977 - val_loss: 0.7533 - val_acc: 0.8415\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 0.0056 - acc: 0.9979 - val_loss: 0.8701 - val_acc: 0.8235\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 3s 417us/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.8578 - val_acc: 0.8605\n"
     ]
    }
   ],
   "source": [
    "history_3_ext = model_3_ext.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ext. model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 144us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8108287644386292, 0.8569999933242798]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_3_ext.metrics_names)\n",
    "model_3_ext.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the above results\n",
    "\n",
    "//ToDo: train the above m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ref.:\n",
    "1. https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
