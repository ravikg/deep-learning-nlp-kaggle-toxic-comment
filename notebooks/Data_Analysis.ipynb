{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "This data analysis is for: Toxic Comment Classification Challenge compeition in Kaggle\n",
    "\n",
    "URL : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "Data files required can be downloaded from here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n",
    "Download and save the data in ./data/ folder relative to this Jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Data\n",
    "Description from the [kaggle website](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data):   \n",
    "You are provided with a large number of Wikipedia comments which have been labeled by human raters for toxic behavior. The types of toxicity are and a comment can have more than 1 label:\n",
    "\n",
    "toxic  \n",
    "severe_toxic  \n",
    "obscene  \n",
    "threat  \n",
    "insult  \n",
    "identity_hate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Analysis\n",
    "\n",
    "1. Read ./data/train.csv file\n",
    "2. Counts no. of training data per class\n",
    "3. A single comment can belong to multiple class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "dataset_dir = './storage/dataset'\n",
    "with ZipFile(f'{dataset_dir}/train.csv.zip', 'r') as zf:\n",
    "    zf.extractall(dataset_dir)\n",
    "with ZipFile(f'{dataset_dir}/test.csv.zip', 'r') as zf:\n",
    "    zf.extractall(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = f'{dataset_dir}/train.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a clean/non-toxic label colum\n",
    "# for column from 2 to 7 (last) it has value 0 and 1,\n",
    "# sum of col. 2 to 7 values for a particular row will result in no. of toxic classes for that row\n",
    "rowsums=train_df.iloc[:,2:].sum(axis=1)\n",
    "train_df['clean']=(rowsums==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar chart for each class label\n",
    "# get count for each class\n",
    "x=train_df.iloc[:,2:].sum()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"# per Class\")\n",
    "plt.ylabel('# of Occurrences', fontsize=12)\n",
    "plt.xlabel('Class ', fontsize=12)\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df['clean'], train_df['toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='clean', data=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "From above analysis we see that:\n",
    "1. Total no. of data (training & validation) = 159,571\n",
    "2. Total non-toxic comments = 143,346 (~ 89.83%)\n",
    "3. Rest 10.17% of commets has one of the toxic label\n",
    "4. Some of labels like threat, identity_hate, severe_toxic has very less samples compared to other labels\n",
    "\n",
    "So this is imbalanced dataset as we can observe that data per class is highly imbalanced"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
