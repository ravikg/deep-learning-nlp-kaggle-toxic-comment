{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classification Challenge\n",
    "\n",
    "This is for : https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "This python notebook uses the tutorial from the book : Deep Learning with Python\n",
    "\n",
    "Data files required can be downloaded from here: https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data\n",
    "\n",
    "Download and save the data in ./data/ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "1. Read ./data/train.csv file\n",
    "2. Counts no. of training data per class\n",
    "3. A single comment can belong to multiple class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'toxic': 15294, 'severe_toxic': 1595, 'obscene': 8449, 'threat': 478, 'insult': 7877, 'identity_hate': 1405, 'non_toxic': 143346}\n",
      "159571\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "labels_count = {'toxic' : 0,\n",
    "                'severe_toxic' : 0,\n",
    "                'obscene' : 0,\n",
    "                'threat' : 0,\n",
    "                'insult' : 0,\n",
    "                'identity_hate' : 0,\n",
    "                'non_toxic' : 0}\n",
    "\n",
    "with open('./data/train.csv') as train_file:\n",
    "    reader = csv.DictReader(train_file)\n",
    "    total_row = 0\n",
    "    for row in reader:\n",
    "        total_row += 1\n",
    "        is_none = True\n",
    "        for label in toxic_labels:\n",
    "            if(row[label] == '1'):\n",
    "                labels_count[label] += 1\n",
    "                is_none = False\n",
    "        if(is_none):\n",
    "            labels_count['non_toxic'] += 1\n",
    "    print(labels_count)\n",
    "    print(total_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 7 artists>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(labels_count.keys(), labels_count.values(), 1/1.5, color='b')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above analysis we see that:\n",
    "1. Total no. of training/validation data = 159,571\n",
    "2. Total non-toxic comments = 143,346 (89.83%)\n",
    "3. Rest 10.17% of commets has one of the toxic label\n",
    "4. Some of labels like threat, identity_hate, severe_toxic has very less samples compared to other labels\n",
    "\n",
    "So data per class is highly imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 (Basic)\n",
    "1. Each comment belongs to only 1 class other than 'toxic' class\n",
    "2. Deep learning network has following layer:  \n",
    " 2.1 Embedding Layer (populated using GloVe model)  \n",
    " 2.2 LSTM Layer  \n",
    " 2.3 Dense Layer to classes\n",
    " \n",
    "This is a basic solution for the simplified version of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the raw data\n",
    "### Create dataset for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "159571\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "toxic_labels_id_map = {\n",
    "    'non_toxic' : 0,\n",
    "    'toxic' : 1,\n",
    "    'severe_toxic' : 2,\n",
    "    'obscene' : 3,\n",
    "    'threat' : 4,\n",
    "    'insult' : 5,\n",
    "    'identity_hate' : 6\n",
    "    }\n",
    "\n",
    "num_rows_texts = 0\n",
    "num_rows_labels = 0\n",
    "with open('./data/train.csv') as train_file:\n",
    "    reader = csv.DictReader(train_file)\n",
    "    for row in reader:\n",
    "        prev_row_count = num_rows_labels\n",
    "        num_rows_texts += 1\n",
    "        texts.append(row['comment_text'])\n",
    "        is_none = True\n",
    "        # check for labels other than toxic (1st label)\n",
    "        for label in toxic_labels[1:]:\n",
    "            if(row[label] == '1'):\n",
    "                num_rows_labels += 1\n",
    "                labels.append(toxic_labels_id_map[label])\n",
    "                is_none = False\n",
    "                break # a comments can have multiple labels but for now just take the 1st label <-- need to fix it\n",
    "        if(is_none):\n",
    "            num_rows_labels += 1\n",
    "            # finally check if it is toxic or non-toxic\n",
    "            if(row['toxic'] == '1'):\n",
    "                labels.append(toxic_labels_id_map['toxic'])\n",
    "            else:\n",
    "                labels.append(toxic_labels_id_map['non_toxic'])\n",
    "        if((num_rows_texts - num_rows_labels) < 0):\n",
    "            print(row)\n",
    "            break;\n",
    "\n",
    "print(num_rows_texts)\n",
    "print(num_rows_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels size: 159571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 2, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('labels size:', len(labels))\n",
    "labels[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoding the data: \n",
    "as it is multi class classification problem, the label need to be encoded into one hot encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "one_hot_labels = to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n",
      "[0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(len(one_hot_labels))\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the data\n",
    "\n",
    "Splitting the comments in to tokens, later used to create word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210337 unique tokes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_words = 10000 # consider only the tops 10,000 words in dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokes.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad/truncate all comments to a fixed length\n",
    "ToDo: do this padding per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (159571, 100)\n",
      "Shape of label tensor: (159571, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "maxlen = 100 # cuts of comments after 100 words\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "one_hot_labels = np.asarray(one_hot_labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "#### Split the data into training data and validation data\n",
    "#### Data can be shuffled (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_samples = 120000 # trains on 120,000 samples\n",
    "validation_samples = 39571 # validates on 39,571 samples (rest)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "one_hot_labels = one_hot_labels[indices]\n",
    "one_hot_labels = one_hot_labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = one_hot_labels[:training_samples]\n",
    "\n",
    "x_val = data[training_samples : training_samples + validation_samples]\n",
    "y_val = one_hot_labels[training_samples : training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe (Global Vector) Word Embedding setup\n",
    "download the glove precomputed imbedding from: https://nlp.stanford.edu/projects/glove\n",
    "unzip it\n",
    "it contains: 100 dimensional embedding vectors for 400,000 words from 2014 English Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the embedding\n",
    "#### Parsing the GloVe word-embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "glove_dir = './data/glove/glove.6B'\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the GloVe word-embedding matrix\n",
    "\n",
    "1. Define embedding matrix dimension = embedding_dim (100) x max_words (10000 : defined above)\n",
    "2. Iterated the word_idex and use top max_words to populated the embedding matrix using glove embedding_index\n",
    "\n",
    "one can play with above params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100 # as we are using 100 dimensional glove index\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, LSTM\n",
    "\n",
    "model = Sequential()\n",
    "# embedding layer\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "# RNN layer (LSTM)\n",
    "model.add(LSTM(32))\n",
    "# output layer (7 classes)\n",
    "model.add(Dense(7, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the GloVe embeddings in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on test data\n",
    "\n",
    "#### Tokenizing the data of the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "test_labels = []\n",
    "test_id = []\n",
    "\n",
    "with open('./data/test.csv') as test_file:\n",
    "    reader = csv.DictReader(test_file)\n",
    "    for row in reader:\n",
    "        test_texts.append(row['comment_text'])\n",
    "        test_id.append(row['id'])\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "x_test = pad_sequences(test_sequences, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels.shape)\n",
    "print(len(test_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the test out in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same the labels only as csv\n",
    "np.savetxt('./test_labels_out.csv', test_labels, delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the test labels with id\n",
    "test_pred_out = []\n",
    "with open('test-result.csv', 'w') as f:\n",
    "    f.write(\"id,toxic,severe_toxic,obscene,threat,insult,identity_hate\\n\")\n",
    "    for i in range(0, len(test_id)):\n",
    "        #print(i)\n",
    "        f.write(test_id[i])\n",
    "        f.write(',')\n",
    "        #generate an array with strings\n",
    "        x_arrstr = np.char.mod('%f', test_labels[i, 1:])\n",
    "        #combine to a string\n",
    "        x_str = \",\".join(x_arrstr)\n",
    "        f.write(x_str)#,0:])#.tolist))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
