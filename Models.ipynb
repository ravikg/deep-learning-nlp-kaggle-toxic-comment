{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We will cover following models:\n",
    "* Embedding => Class\n",
    "* Embedding => Simple RNN => Class\n",
    "* Embedding => Bi-directional RNN => Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the toxic comment classification challenge dataset\n",
    "and split the dataset into training, validation, testing\n",
    "#### Input text for training\n",
    "1. Get the training data\n",
    "    * read the csv data file using pandas\n",
    "    * tokenize the data\n",
    "    * assign a dimension to each word\n",
    "    * convert into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_csv = './data/toxic-comments/train.csv'\n",
    "train_df = pd.read_csv(train_csv)\n",
    "# ToDo : sort the df based on size of comments (no. of words in comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowsums=train_df.iloc[:,2:].sum(axis=1)\n",
    "train_df['clean']=(rowsums==0)\n",
    "train_texts = train_df['comment_text']\n",
    "train_labels = train_df['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output labels (class/target) for training\n",
    "2. Get the training label\n",
    "    * read the labels and convert into one-class labels\n",
    "    * We will focus on 2 class problem: toxic and non toxic comments\n",
    "    * We will label all different types of toxic comments into same category of toxic label:\n",
    "        * 0 for toxic comment\n",
    "        * 1 for non-toxic comments\n",
    "    * Later we can explore how to make it multiclass classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "Now we have training data in two separate array: an ordered array consisting of comments (input for the network) and another array consisting of class lables in same order (output of the network).\n",
    "\n",
    "We have to transform this data into network input format and output format. This step is called pre-processing.  \n",
    "Steps of preprocessing:\n",
    "\n",
    "1. Tokenize the text into words\n",
    "2. Assign each word a dimension\n",
    "\n",
    "\n",
    "To accompolish step 1 and 2 we will use inbuilt Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 1116, 86, 331, 51, 2278, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n",
      "Found 210337 unique tokes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokes.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching and Preprocessing (padding) for Embedding\n",
    "Now once we have the tokens, we will do following steps to create word embeddings  \n",
    "\n",
    "3. Then use this dimension assignment to define embedding\n",
    "4. Use word embedding to greate word vector for a comment\n",
    "\n",
    "\n",
    "We will use a specific type of Layer for this, which is called Embedding Layer. The above generated tokens will go as input to Embedding layer, which will output word embeddings as output to next layer:  \n",
    "\n",
    "   **Input**: 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers (output of above code).  \n",
    "   **Output**: 3D floating-point tensor of shape (samples, sequence_length, embedding_dimensionality).  \n",
    "\n",
    "Sequence length can be variable per batch. But in a single batch sequence length will be same for all sequences.  \n",
    "\n",
    "So from data we have to create batches of sequence of similar length and then pad or truncate each sequence to have same sequence length within a particular batch. And we can use each batch as a training input for embedding layer.  \n",
    "\n",
    "For sample case: we take 10k sequence from 160k for training in a single batch. And take max sequence length of 50 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import preprocessing\n",
    "training_sequences = sequences[:10000]\n",
    "training_labels = train_labels[:10000]\n",
    "seq_max_len = 20\n",
    "# training padded sequences\n",
    "train_seq_pad = preprocessing.sequence.pad_sequences(sequences=training_sequences, maxlen=seq_max_len)\n",
    "\n",
    "# testing padded sequences\n",
    "testing_sequences = sequences[10000:11000]\n",
    "testing_labels = train_labels[10000:11000]\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=testing_sequences, maxlen=seq_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. : Embedding to Class\n",
    "\n",
    "#### Define the model 1\n",
    "Model 1 is made of 4 layers:\n",
    "    - Layer 0 is input layer\n",
    "    - Layer 1 is Embedding layer (Hidden Layer)\n",
    "    - Layer 2 is Flatten Layer (Flattens the embedding layer)\n",
    "    - Layer 3 is Dense Layer (output layer)\n",
    "    \n",
    "**Embedding Layer**: This layer help us create word embedding (discussed in Sequence Representation section). For a single input (a sentence which comes as a seq. of integer) its output is 2D. Each integer(representing a word) gets transformed into a vector; so for a seq. of int. it generates a 2D matrix.\n",
    "\n",
    "**Flatten Layer**: Embedding layer outputs in 2D matrix, to use the output in a Dense layer upstream the output need to transformed into 1D and flatten layer does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/.virtualenvs/deep-learn2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 0s 46us/step - loss: 0.4607 - acc: 0.8799 - val_loss: 0.2831 - val_acc: 0.9100\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 0.2788 - acc: 0.8956 - val_loss: 0.2437 - val_acc: 0.9120\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 0s 38us/step - loss: 0.2362 - acc: 0.9029 - val_loss: 0.2230 - val_acc: 0.9185\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.2060 - acc: 0.9180 - val_loss: 0.2123 - val_acc: 0.9265\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1872 - acc: 0.9268 - val_loss: 0.2082 - val_acc: 0.9315\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.1748 - acc: 0.9330 - val_loss: 0.2073 - val_acc: 0.9335\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1651 - acc: 0.9373 - val_loss: 0.2074 - val_acc: 0.9355\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 0s 39us/step - loss: 0.1567 - acc: 0.9409 - val_loss: 0.2077 - val_acc: 0.9335\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 0s 41us/step - loss: 0.1485 - acc: 0.9430 - val_loss: 0.2091 - val_acc: 0.9340\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 0s 40us/step - loss: 0.1420 - acc: 0.9445 - val_loss: 0.2093 - val_acc: 0.9360\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "# no. of unique words in the text data, each word in vocab will be assigned an index (dimension).\n",
    "vocab_size = 10000 \n",
    "\n",
    "# max length of single input data point i.e. count of words present in an input sentence\n",
    "# short seq are padded and long ones are truncated, done above\n",
    "# input of the network\n",
    "seq_max_len = 20 \n",
    "\n",
    "# dimension of word embedding model (output dimension of embedding layer)\n",
    "embedding_dim = 8 \n",
    "# input to layer 0 is data of shape: [batch_size, seq_max_len]\n",
    "# add layer 1 in the network\n",
    "model_1.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "# output of layer 1 is data of shape: [batch_size, embedding_dim, seq_max_len]\n",
    "\n",
    "## layer 2: flatten the input of shape [batch_size, embedding_dim, seq_max_len] \n",
    "#          to output of shape [batch_size, embedding_dimension*seq_max_len]\n",
    "model_1.add(Flatten())\n",
    "\n",
    "## layer 3(output layer): Dense layer - all nodes from previous layers are connected to each nodes from this layer\n",
    "#          this has 1 unit/node for classification(toxic/non-toxic)\n",
    "#          and activation for 2 classes: sigmoind\n",
    "model_1.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "## compile: configure the model for training\n",
    "# optimizer: it is the method use to update the network, \n",
    "#            it is generally variant of stochastic gradient descent (SGD)  \n",
    "#            this method is use iteratively to update the network weights\n",
    "# loss:      it is the (objective) function that will be minimised\n",
    "# metrics:   this is use to measure the performance of network\n",
    "model_1.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# prints the summary of the model\n",
    "model_1.summary()\n",
    "\n",
    "# fit: trains the network for a fixed no. of epoch\n",
    "history_1 = model_1.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/m11.png\" alt=\"Visual representation of one hot encodding and word embedding\" style=\"width: 600px;\"/>\n",
    "\n",
    "\n",
    "For above diagram, following configs are used(1/4th of the ones used in code):\n",
    "1. seq_max_len = 5\n",
    "2. embedding_dim = 2\n",
    "3. flatten layer = 5x2 = 10\n",
    "4. desnse output layer = 1\n",
    "\n",
    "Created using [NN SVG Tool](http://alexlenail.me/NN-SVG/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model 1\n",
    "\n",
    "We will take a small test data from the unused training data to test our basic model.  \n",
    "\n",
    "`model_1.evaluate` method is use to evaluate the model. For evaluation we give input the test data in the same format as of training data together with label data for the test data to compare with.\n",
    "\n",
    "Ref: Listing 6.7 Deep Learning with Python book  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19870102918148042, 0.925000011920929]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_1.metrics_names)\n",
    "model_1.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding => RNN => Output\n",
    "In this model 2 we will extend the Model 1 by adding an RNN layer in between the Embedding layer and output layer.\n",
    "\n",
    "#### Define the model 2\n",
    "Model 2 is made of 3 layers:\n",
    "    - Layer 1 is Embedding layer\n",
    "    - Layer 2 is RNN layer\n",
    "    - Layer 3 is classification (Dense) layer \n",
    "    \n",
    "**RNN** : Recurrent Neural Network\n",
    "\n",
    "\n",
    "<img src=\"img/rnn.png\" alt=\"Recurrent Neural Network\" style=\"width: 300px;\"/>\n",
    "\n",
    "RNN is a neural network has following properties:\n",
    " - processes each element(word) of a sequence(sentence) one by one \n",
    " - and output of intermediate element is fed back together with the next element.\n",
    " - The state of RNN is reset between two indepeent sequence\n",
    " \n",
    "Input for Dense layer is only the output at the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Unrolled RNN](img/RNN-unrolled.png)\n",
    "Source: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, we don't need Flatten layer as by default SimpleRNN layer output only the last element from the processed output (h_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                1568      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 161,601\n",
      "Trainable params: 161,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16\n",
    "\n",
    "# model definition\n",
    "model_2 = Sequential()\n",
    "model_2.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "model_2.add(SimpleRNN(32))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "model_2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/.virtualenvs/deep-learn2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.3253 - acc: 0.8915 - val_loss: 0.2570 - val_acc: 0.9165\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.2211 - acc: 0.9191 - val_loss: 0.2177 - val_acc: 0.9340\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.1607 - acc: 0.9423 - val_loss: 0.2298 - val_acc: 0.9180\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 1s 129us/step - loss: 0.1213 - acc: 0.9561 - val_loss: 0.2138 - val_acc: 0.9295\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.0841 - acc: 0.9693 - val_loss: 0.2236 - val_acc: 0.9330\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.0560 - acc: 0.9804 - val_loss: 0.3116 - val_acc: 0.8895\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 133us/step - loss: 0.0360 - acc: 0.9864 - val_loss: 0.3027 - val_acc: 0.9000\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 130us/step - loss: 0.0224 - acc: 0.9934 - val_loss: 0.4068 - val_acc: 0.8720\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 131us/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.4400 - val_acc: 0.8865\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 128us/step - loss: 0.0108 - acc: 0.9973 - val_loss: 0.4299 - val_acc: 0.8875\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 38us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4116805019080639, 0.8820000290870667]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2.metrics_names)\n",
    "model_2.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that above model didn't have good accuracy compared to much simpler model. We didn't use most of the data, training data is very less and also value of seq_len was less for training data and more for testing data.\n",
    "\n",
    "\n",
    "We can extend the model by adding more RNN layers in between and for the above we didn't use the out of intermediate output of RNN layer.\n",
    "\n",
    "#### Extended model 2\n",
    "Extended model 2 is made of 5 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is RNN layer (return full sequence)\n",
    "- Layer 3 is RNN layer (return full sequence)\n",
    "- Layer 4 is RNN layer (return last output)\n",
    "- Layer 5 is classification (Dense) layer  \n",
    "\n",
    "In this setup we have to pass full processed output for all but last RNN layer.  \n",
    "For above model 1, it is many to one  \n",
    "For model 2, it is many to many from below diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN types](img/rnn.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 32)          1568      \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 64)          6208      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 32)                3104      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 170,913\n",
      "Trainable params: 170,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2_ext = Sequential()\n",
    "model_2_ext.add(Embedding(vocab_size, embedding_dim))\n",
    "# for intermediate layers, we want to return output of each cell of RNN, \n",
    "# so that it forms a seq. which is processed by next RNN layer\n",
    "model_2_ext.add(SimpleRNN(32, return_sequences=True))\n",
    "model_2_ext.add(SimpleRNN(64, return_sequences=True))\n",
    "# in final RNN layer we will not return the sequence but only the final output,\n",
    "# which is use in the next non RNN layer e.g. Dense layer in this case\n",
    "model_2_ext.add(SimpleRNN(32))\n",
    "model_2_ext.add(Dense(1, activation='sigmoid'))\n",
    "model_2_ext.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_2_ext.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the ext. model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/.virtualenvs/deep-learn2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 3s 345us/step - loss: 0.2990 - acc: 0.8972 - val_loss: 0.2500 - val_acc: 0.9170\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 2s 276us/step - loss: 0.1887 - acc: 0.9325 - val_loss: 0.2069 - val_acc: 0.9375\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 0.1306 - acc: 0.9556 - val_loss: 0.2327 - val_acc: 0.9225\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2s 269us/step - loss: 0.0805 - acc: 0.9715 - val_loss: 0.2577 - val_acc: 0.9305\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.0424 - acc: 0.9862 - val_loss: 0.3250 - val_acc: 0.9115\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.0208 - acc: 0.9934 - val_loss: 0.3738 - val_acc: 0.9175\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.0115 - acc: 0.9959 - val_loss: 0.7841 - val_acc: 0.8280\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.0072 - acc: 0.9975 - val_loss: 0.5345 - val_acc: 0.9125\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 266us/step - loss: 0.0031 - acc: 0.9990 - val_loss: 0.8282 - val_acc: 0.8540\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s 267us/step - loss: 7.3933e-04 - acc: 0.9998 - val_loss: 0.8810 - val_acc: 0.8775\n"
     ]
    }
   ],
   "source": [
    "history_2_ext = model_2_ext.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the ext. model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 85us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9305257204920053, 0.8619999885559082]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_2_ext.metrics_names)\n",
    "model_2_ext.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Embedding => Bidirectional RNN => Output\n",
    "In this model 3 we will extend the Model 2 by wrapping the RNN layer with a Bidirectional wrapper.\n",
    "\n",
    "#### Define the model 3\n",
    "Extended model 3 is made of 3 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is Bidirectional RNN layer (return last output)\n",
    "- Layer 3 is classification (Dense) layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                3136      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 163,201\n",
      "Trainable params: 163,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16\n",
    "\n",
    "# model definition\n",
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "# [1] This will create two copies of the hidden layer, \n",
    "# one fit in the input sequences as-is and one on a reversed copy of the input sequence. \n",
    "# By default, the output values from these LSTMs will be concatenated.\n",
    "model_3.add(Bidirectional(SimpleRNN(32)))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "model_3.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/.virtualenvs/deep-learn2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.2766 - acc: 0.9056 - val_loss: 0.2098 - val_acc: 0.9365\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.1807 - acc: 0.9389 - val_loss: 0.1912 - val_acc: 0.9415\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.1416 - acc: 0.9510 - val_loss: 0.2039 - val_acc: 0.9360\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.1089 - acc: 0.9622 - val_loss: 0.2132 - val_acc: 0.9325\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.0827 - acc: 0.9710 - val_loss: 0.2134 - val_acc: 0.9360\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.0616 - acc: 0.9791 - val_loss: 0.2401 - val_acc: 0.9265\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.0421 - acc: 0.9869 - val_loss: 0.2912 - val_acc: 0.9140\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.0299 - acc: 0.9901 - val_loss: 0.3329 - val_acc: 0.9085\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.0210 - acc: 0.9931 - val_loss: 0.3318 - val_acc: 0.9140\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 2s 192us/step - loss: 0.0145 - acc: 0.9956 - val_loss: 0.3773 - val_acc: 0.9005\n"
     ]
    }
   ],
   "source": [
    "history_3 = model_3.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 57us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.35309847700595853, 0.9010000228881836]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_3.metrics_names)\n",
    "model_3.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly like model 2, model 3 can be extended by adding more bidirectional layers in between.  \n",
    "\n",
    "#### Extended model 3\n",
    "Extended model 3 is made of 5 layers:\n",
    "\n",
    "- Layer 1 is Embedding layer\n",
    "- Layer 2 is Bidirectional RNN layer (return full sequence)\n",
    "- Layer 3 is Bidirectional RNN layer (return full sequence)\n",
    "- Layer 4 is Bidirectional RNN layer (return last output)\n",
    "- Layer 5 is classification (Dense) layer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, SimpleRNN\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "\n",
    "# model configurations\n",
    "vocab_size = 10000\n",
    "seq_max_len = 20 # this can be removed as it is not required for next layer which is RNN\n",
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 16)            160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 20, 64)            3136      \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 20, 128)           16512     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                10304     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 190,017\n",
      "Trainable params: 190,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "model_3_ext = Sequential()\n",
    "model_3_ext.add(Embedding(vocab_size, embedding_dim, input_length=seq_max_len))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(32, return_sequences=True)))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(64, return_sequences=True)))\n",
    "model_3_ext.add(Bidirectional(SimpleRNN(32)))\n",
    "model_3_ext.add(Dense(1, activation='sigmoid'))\n",
    "model_3_ext.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model_3_ext.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train ext. model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ravi/.virtualenvs/deep-learn2/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 5s 580us/step - loss: 0.3226 - acc: 0.8940 - val_loss: 0.3720 - val_acc: 0.8275\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 3s 432us/step - loss: 0.2063 - acc: 0.9262 - val_loss: 0.2138 - val_acc: 0.9270\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 4s 442us/step - loss: 0.1268 - acc: 0.9557 - val_loss: 0.2285 - val_acc: 0.9285\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 3s 428us/step - loss: 0.0755 - acc: 0.9745 - val_loss: 0.2589 - val_acc: 0.9245\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 3s 423us/step - loss: 0.0428 - acc: 0.9859 - val_loss: 0.3148 - val_acc: 0.8985\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 3s 427us/step - loss: 0.0226 - acc: 0.9931 - val_loss: 0.3941 - val_acc: 0.8880\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 3s 430us/step - loss: 0.0127 - acc: 0.9964 - val_loss: 0.3898 - val_acc: 0.9110\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 3s 426us/step - loss: 0.0082 - acc: 0.9965 - val_loss: 0.4683 - val_acc: 0.8925\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 3s 426us/step - loss: 0.0061 - acc: 0.9983 - val_loss: 0.6040 - val_acc: 0.8630\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 3s 426us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.6943 - val_acc: 0.9020\n"
     ]
    }
   ],
   "source": [
    "history_3_ext = model_3_ext.fit(train_seq_pad, training_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test ext. model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 147us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7279831589460373, 0.8999999761581421]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_3_ext.metrics_names)\n",
    "model_3_ext.evaluate(x=test_seq_pad, y=testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the above results\n",
    "\n",
    "//ToDo: train the above m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ref.:\n",
    "1. https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
