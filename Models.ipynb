{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "We will cover following models:\n",
    "* Embedding => Class\n",
    "* Embedding => Simple RNN => Class\n",
    "* Embedding => Bi-directional RNN => Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Load the toxic comment classification challenge dataset\n",
    "and split the dataset into training, validation, testing\n",
    "#### Training input text\n",
    "1. Get the training data\n",
    "    * read the csv data file\n",
    "    * tokenize the data\n",
    "    * assign a dimension to each word\n",
    "    * convert into embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data_folder = './data/toxic-comments/'\n",
    "train_texts = []\n",
    "with open(data_folder+'train.csv') as train_file:\n",
    "    reader = csv.DictReader(train_file)\n",
    "    for row in reader:\n",
    "        train_texts.append(row['comment_text'])\n",
    "print(len(train_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training output labels\n",
    "2. Get the training label\n",
    "    * read the labels and convert into one-class labels\n",
    "    * We will focus on 2 class problem: toxic and non toxic comments\n",
    "    * We will label all different types of toxic comments into same category of toxic label:\n",
    "        * 0 for toxic comment\n",
    "        * 1 for non-toxic comments\n",
    "    * Later we can explore how to make it multiclass classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "toxic_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "with open(data_folder+'train.csv') as train_file:\n",
    "    reader = csv.DictReader(train_file)\n",
    "    for row in reader:\n",
    "        not_toxic = True\n",
    "        # check for toxic labels\n",
    "        for label in toxic_labels:\n",
    "            if(row[label] == '1'):\n",
    "                train_labels.append(0)\n",
    "                not_toxic = False\n",
    "                break\n",
    "        if not_toxic:\n",
    "            train_labels.append(1)\n",
    "\n",
    "print(len(train_labels))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "Now we have training data in two separate array: an ordered array consisting of comments (input) and another array consisting of class lables in same order (output).\n",
    "\n",
    "We have transform this data into network input format and output format.\n",
    "Steps of preprocessing:\n",
    "\n",
    "1. Tokenize the text into words\n",
    "2. Assign each word a dimension\n",
    "\n",
    "\n",
    "To accompolish step 1 and 2 we will use inbuilt Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[688, 75, 1, 126, 130, 177, 29, 672, 4511, 1116, 86, 331, 51, 2278, 50, 6864, 15, 60, 2756, 148, 7, 2937, 34, 117, 1221, 2825, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 143, 73, 3462, 89, 3085, 4583, 2273, 985]\n",
      "Found 210337 unique tokes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "max_vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "print(sequences[0])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokes.' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching and Preprocessing (padding) for Embedding\n",
    "Now once we have the tokens, we will do following steps to create word embeddings  \n",
    "\n",
    "3. Then use this dimension assignment to define embedding\n",
    "4. Use word embedding to greate word vector for a comment\n",
    "\n",
    "\n",
    "We will use a specific type of Layer for this, which is called Embedding Layer. The above generated tokens will go as input to Embedding layer, which will output word embeddings as output to next layer:  \n",
    "\n",
    "   **Input**: 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers (output of above code).  \n",
    "    **Output**: 3D floating-point tensor of shape (samples, sequence_length, embedding_dimensionality).  \n",
    "\n",
    "Sequence length can be variable per batch. But in a single batch sequence length will be same for all sequences.  \n",
    "\n",
    "So from data we have to create batches of sequence of similar length and then pad or truncate each sequence to have same sequence length within a particular batch. And we can use each batch as a training input for embedding layer.  \n",
    "\n",
    "For sample case: we take 10k sequence from 160k for training in a single batch. And take max sequence length of 50 words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sequences = sequences[:10000]\n",
    "sample_labels = train_labels[:10000]\n",
    "seq_max_len = 50\n",
    "\n",
    "from keras import preprocessing\n",
    "\n",
    "train_seq_pad = preprocessing.sequence.pad_sequences(sequences=sample_sequences, maxlen=max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0   52 2635   13  555 3809   73 4556 2706   21   94   38\n",
      "  803 2679  992  589 8377  182]\n"
     ]
    }
   ],
   "source": [
    "print(train_seq_pad[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1. : Embedding to Class\n",
    "\n",
    "#### Define the model\n",
    "Our model is made of 2 layers. Layer 1 is embedding layer\n",
    "Layer 2 a classification Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n",
      "8000/8000 [==============================] - 1s 137us/step - loss: 0.4397 - acc: 0.8821 - val_loss: 0.2760 - val_acc: 0.9095\n",
      "Epoch 2/10\n",
      "8000/8000 [==============================] - 1s 63us/step - loss: 0.2767 - acc: 0.8954 - val_loss: 0.2419 - val_acc: 0.9120\n",
      "Epoch 3/10\n",
      "8000/8000 [==============================] - 1s 75us/step - loss: 0.2343 - acc: 0.9051 - val_loss: 0.2209 - val_acc: 0.9205\n",
      "Epoch 4/10\n",
      "8000/8000 [==============================] - 0s 62us/step - loss: 0.2054 - acc: 0.9183 - val_loss: 0.2106 - val_acc: 0.9290\n",
      "Epoch 5/10\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.1868 - acc: 0.9279 - val_loss: 0.2075 - val_acc: 0.9300\n",
      "Epoch 6/10\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.1755 - acc: 0.9330 - val_loss: 0.2057 - val_acc: 0.9340\n",
      "Epoch 7/10\n",
      "8000/8000 [==============================] - 1s 64us/step - loss: 0.1661 - acc: 0.9375 - val_loss: 0.2058 - val_acc: 0.9340\n",
      "Epoch 8/10\n",
      "8000/8000 [==============================] - 1s 73us/step - loss: 0.1587 - acc: 0.9402 - val_loss: 0.2063 - val_acc: 0.9350\n",
      "Epoch 9/10\n",
      "8000/8000 [==============================] - 1s 66us/step - loss: 0.1522 - acc: 0.9423 - val_loss: 0.2059 - val_acc: 0.9360\n",
      "Epoch 10/10\n",
      "8000/8000 [==============================] - 1s 65us/step - loss: 0.1455 - acc: 0.9443 - val_loss: 0.2066 - val_acc: 0.9355\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "# layer 0: add an embedding layer:\n",
    "vocab_size = 10000 # no. of unique words in the text data, each word in vocab will be assigned an index (dimension).\n",
    "embedding_dim = 8 # dimension of word embedding model, output of this layer\n",
    "max_len = 20 # max length of single input data e.g. count of words present in an input sentence, input of this layer\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "# input to above layer will be data of shape: [batch_size, max_len]\n",
    "# output of above layer will be data of shape: [batch_size, embedding_dimension, max_len]\n",
    "# layer 1: flatten the input of shape [batch_size, embedding_dimension, max_len] \n",
    "#          to out of shape [batch_size, embedding_dimension*max_len]\n",
    "model.add(Flatten())\n",
    "# layer 2: Dense layer - all nodes from previous layers are connected to each nodes from this layer\n",
    "#          this has 1 unit/node for classification; and activation for 2 classes: sigmoind\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile: configure the model for training\n",
    "#   optimizer: it is the method use to update the network, it is generally variant of stochastic gradient descent (SGD)  \n",
    "#              this method is use iteratively to update the network weights\n",
    "#   loss: it is the (objective) function that will be minimised\n",
    "#   metrics: this is use to measure the performance of network\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "# todo: check this method\n",
    "model.summary()\n",
    "# fit: trains the network for a fixed no. of epoch\n",
    "history = model.fit(train_seq_pad, sample_labels, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'acc']\n",
      "1000/1000 [==============================] - 0s 35us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1994455663561821, 0.927]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequences = sequences[10000:11000]\n",
    "test_labels = train_labels[10000:11000]\n",
    "seq_max_len = 50\n",
    "test_seq_pad = preprocessing.sequence.pad_sequences(sequences=test_sequences, maxlen=max_len)\n",
    "\n",
    "print(model.metrics_names)\n",
    "model.evaluate(x=test_seq_pad, y=test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: Listing 6.7 Deep Learning with Python  \n",
    "\n",
    "//todo explain above code and add network diagram\n",
    "Embedding("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding => RNN => Output\n",
    "In this model 2 we will extend the Model 1 by adding an RNN layer in between the Embedding layer and output layer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
