{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text can be seen as following:\n",
    "* Sequence of characters\n",
    "* Sequence of words\n",
    "* Sequence of N-grams\n",
    "* Most common to work at the level of words\n",
    "\n",
    "Using this representation of text, most of the models understand the statistical structure of text i.e. identify various features from the text which can solve some of the textual tasks e.g. document classification, author identificaiton, sentiment analysis etc. At high level, various algorithms treat the sequence elements similarly to how pixels are treated in context of images in computere vision.\n",
    "\n",
    "### Numerical representation of text as sequence\n",
    "All algorithms works on numerial tensor, so we need to transform the raw text sequence into numerical tensor.\n",
    "\n",
    "#### Tokenization\n",
    "A text can be split in lower form by splitting into seq. of char or words or n-grams. This process of splitting is called tokenization and individual elements like char, words or n-gram is called token. And then each token can be converted into a numerical vector:\n",
    "\n",
    "* Sequence of chars: treat each char in the sequence as a vector and using that create a tensor for the whole sequence of char\n",
    "* Seq. of words: treat each word as a vector and using that create a tensor for a single sentence.\n",
    "* Split the sentence using n-gram(take consecutive n chars as a single entity) and treat each n-gram as a vector and then create a numerial tensor out of it.\n",
    "\n",
    "\n",
    "#### Vector representation\n",
    "* One hot encoding\n",
    "* Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding\n",
    "* It is the most basic way to transform a token into a vector\n",
    "* Total no. of dimension = no. of unique tokens in vocabulary\n",
    "* Assign each token a unique dimension and then represent that token by a vector with value 1 at the assigned dimension and 0 in other dimension.\n",
    "* Here the tokens can be chars, words, n-grams\n",
    "* Vectors are binary, sparse (mostly made of 0), high dimensional e.g. 20,000 dimensional for data having that much unique tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//todo\n",
    "add image\n",
    "add keras code snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Embeddings\n",
    "* More powerful way of representing tokens as vectors\n",
    "* Here tokens are words\n",
    "* Embeddings are learned from data\n",
    "* Word embedding are low dimensional floating point vectors. Generally used dimensions are: 256, 512, 1024..\n",
    "* Two ways:\n",
    "    * Learn with the main task\n",
    "        * Start with random vector for a token and then learn\n",
    "    * Load pre trained word embedings computed in different task\n",
    "* Features:\n",
    "    * Geometric relationship with vectors, should reflect semantic relationship between words\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "//todo\n",
    "add image\n",
    "add keras code snippet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
